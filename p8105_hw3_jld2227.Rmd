---
title: "HW3"
author: "James Dalgleish"
date: "October 9, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```
First, we'll begin by importing the dataset, cleaning the names to be more appropriate, and
focusing on the Overall Health topic (by filtering for this topic only in the BRFSS responses).
Responses are filtered to only include the 5 ordinal categories, Excellent, Very good,
Good, Fair, Poor.
```{r data_import, results="asis"}
brfss <- p8105.datasets::brfss_smart2010 %>% #Pulls dataframe out of package.
  janitor::clean_names() %>%  #Converts to snake case.
  filter(topic == "Overall Health",
         response %in% c("Excellent","Very good", "Good", "Fair", "Poor")) %>%     #Filters by overall health topic.
  mutate(response = recode(response,"Very good" = "Very Good")) %>% 
  mutate(response = factor(response,levels = c("Excellent","Very Good", "Good", "Fair", "Poor")))
n_7 <- brfss %>% 
  filter(year == 2002) %>% 
  group_by(locationabbr) %>% 
count(locationabbr) %>% 
  arrange(-n) %>% 
filter(n == 7)
  
```
"In 2002, which states were observed at 7 locations?""
Clearly, from the table, `r n_7` were observed at exactly 7 locations. `r n_7` had observations at exactly 7 locations in any year.
Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010."
```{r spaghetti_plot}
spaghetti <- brfss %>% 
  filter(year >= 2002 & year <= 2010) %>%
  group_by(locationabbr,year) %>% 
  summarise(n = n()) %>% 
  ggplot(aes(x = year, y = n, color = locationabbr)) +
  geom_line() + 
  scale_color_viridis_d(option = "viridis") +
  labs(x = "Year",y = "Number of locations in each State",
       color = "State",
       title = "Number of locations in each state, by year, 2002-2010") +
  theme_dark()
  #+ geom_text(aes(x=locationabbr)) 
print(spaghetti)
```

"Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State."
The proportion of the response (in percent) is located in the data_value column, filtering by 2002,2006,2010, for NY and for Excellent responses only. The proportion therefore can be obtained by dividing by 100, from which point the mean and sd can be taken as summary statistics after grouping. 
```{r table_mean_ex,results='asis'}
statewide_ex_stats <- brfss %>% 
  filter(year %in% c(2002,2006,2010),
         locationabbr == "NY",response=="Excellent") %>%
  group_by(locationabbr,year) %>% 
  summarize(ex_sd_prop = 
  sd(data_value/100),
  ex_mean_prop = mean(data_value/100))  %>% 
    select(year,locationabbr,ex_sd_prop,ex_mean_prop)
statewide_ex_stats
```

"For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time."
```{r}
brfss %>% 
  group_by(locationabbr,response,year) %>%
    summarize(sd_prop = 
  sd(data_value/100,na.rm = T),
  mean_prop = mean(data_value/100,na.rm = T)) %>% 
  ggplot(aes(x=year,y=mean_prop,facet=response,color=response,fill=response)) +
  geom_violin() + geom_point(alpha = 0.3) +
  scale_color_viridis_d(option = "magma") +
  scale_fill_viridis_d(option = "viridis") +
  facet_grid(.~response)

```

#Problem 2

```{r instacart_import}
instacart <- p8105.datasets::instacart %>% #Pulls dataframe out of package.
  janitor::clean_names() #cleans names
```
```{r distinct_aisles}
distinct_aisles <- instacart %>%
  group_by(aisle) %>% 
  summarize(count = n()) %>% 
  arrange(
    desc(count)
  )

```
There appear to be `r nrow(distinct_aisles)` aisles. The aisles with the most are displayed in the table below with their counts:
```{r}
distinct_aisles %>% 
  head() %>% 
  kable()
```

The numbers of items in each aisle can be visualized with the following plot. The number of aisles is `r nrow(distinct_aisles)`, making it difficult to label all of the data (although key data is properly labeled in the table above). The previous table shows that `r distinct_aisles %>% pull(aisle) %>% slice(1)` and `r distinct_aisles %>% pull(aisle) %>% slice(2)` are the outliers near 15000, with `r distinct_aisles %>% pull(aisle) %>% slice(3)` at roughly half the magnitude. The top 5 items are labeled, matching the prior table.

```{r items_by_aisle_plot}
items_by_aisle <- instacart %>%
  group_by(aisle) %>% 
count() %>% 
  arrange(
    desc(n)
    ) %>%
as.data.frame() %>% #makes the grouping variable modifyable 
  as.tibble() %>%  #factor reorder.
  mutate(aisle = forcats::fct_reorder(aisle, n))

library(ggrepel)
items_by_aisle %>% 
  ggplot(aes(x = aisle,y = n,fill = n)) + #establishes mappings.
  geom_col() + #like geom_bar, but takes a variable for height.
#  scale_fill_viridis_c() +
  scale_fill_viridis_c() + #adds color
  scale_y_continuous(expand = c(0,0)) + #removes whitespace at bottom.
  #scale_x_discrete(breaks=levels(items_by_aisle$aisle)[c(130,134)]) + #labels=items_by_aisle$aisle[c(130,134)],
   theme_bw() +
  geom_text_repel(data = items_by_aisle %>% head(n=5),aes(label = aisle,color = NULL)) +
  theme(#axis.title.x = element_blank(), #removes panel grid at back.
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  labs(x = "aisles, sorted by number of items",
         y = "Items in aisle",
       fill = "Items in aisle",
       title = "Sorted Instacart Items by Aisle")

```

Now, we'll create a table with the most popular items in the aisles for
baking ingredients, dog food care, and packaged vegetables/fruits.
```{r}
instacart %>% 
  filter(aisle
         %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
count(product_name) %>% 
  filter(n == max(n)) %>%
  arrange(
    desc(n)
  ) %>% 
  select(product_name,n,everything())

```

Instruction"Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)."
Now, we'll show the mean hour of the day at which the items are ordered.
```{r}
pink_ice_tbl<-instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour=mean(order_hour_of_day)) %>% 
  spread(key=product_name,value=,mean_hour) %>% 
mutate(order_dow = recode(.$order_dow,"0" = "Sunday",
                             "1" = "Monday",
                             "2" = "Tuesday",
                             "3" = "Wednesday",
                             "4" = "Thursday",
                             "5" = "Friday",
                             "6" = "Saturday")) %>% 
  janitor::clean_names() %>% 
  select(order_dow,
         coffee_ice_cream,
         pink_lady_apples)
pink_ice_tbl
```

#Problem 3

```{r noaa_data_import_for_inspection}
noaa <- p8105.datasets::ny_noaa %>% 
  as.tibble() %>% 
  janitor::clean_names()
  
```
"The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):"
This NOAA dataset for the new york area from `r noaa %>% distinct(id) %>% nrow()` stations has `r noaa %>% nrow` rows and `r noaa %>% ncol` columns,   which columns include total daily precipitation (prcp), miminum temperature, maximum temperature, snowfall,snowdepth at each station.
```{r missing_data}
prop.na<-function(x){
   is.na(x) %>% mean()
}
sapply(noaa,  prop.na)
```
A quick look across the column shows that there is significant missing data (which aligns with the missing data reported in the dataset description,http://p8105.com/dataset_noaa.html), almost half of the data in the maximum temperature and minimum temperature are missing. The cause of this is also detailed in the description, specifically that each station can only record some of the datapoints in the dataset (and the datapoints that a particular station cannot record are missing).  From documentation (https://www1.ncdc.noaa.gov/pub/data/ghcn/daily/readme.txt), we also note that the temperatures are in tenths of a degree celcius, we'll correct for this by dividing by 10.  Precipitation is in tenths of a milimeter, while snofall is in mm. We'll also correct this by dividing by 10.


Instruction: "Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why? Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?"
```{r noaa_clean}
noaa_clean <- p8105.datasets::ny_noaa %>% 
  as.tibble() %>% 
  janitor::clean_names() %>% 
mutate(day = lubridate::day(noaa$date),
       month = lubridate::month(noaa$date),
       year = lubridate::year(noaa$date),
       tmax = as.numeric(tmax)/10,
       tmin = as.numeric(tmin)/10,
       prcp = as.numeric(prcp)/10,
       snow = as.numeric(snow),
       snwd = as.numeric(snwd)) 
#most common values for snowfall
common_snow_vals <- noaa_clean %>% 
  count(snow) %>% 
  arrange(-n)
common_vals_string<-common_snow_vals %>%
  pull(snow) %>%
  head() %>%
  paste(sep = ',')
```

The five most common values include: `r common_vals_string`. The reason for the large amount of snowfall missing is due to the fact that stations only record a subset of the variables and the reason there are are many days with zero is likely because there are many days in the year in new york with zero snowfall (particularly in the summer, spring and fall months).
```{r two_panel_avg_max_temp}
noaa_clean %>% 
  filter(month %in% c(1,7)) %>%
  mutate(month = recode(month, "1" = "January", "7" = "July")) %>% 
  group_by(year,month) %>%
  summarize(mean_tmax = mean(tmax,na.rm = TRUE)) %>% 
  ggplot(aes(x=year,y=mean_tmax,facet=month)) +
    geom_hex() +
  facet_grid(.~month) +
  labs(x="Year",y="Mean temperature")

# noaa_clean %>% 
#   filter(month %in% c(1,7)) %>%
#   mutate(month = recode(month, "1" = "January", "7" = "July")) %>% 
#   group_by(year,month) %>%
#   ggplot(aes(x=year,y=tmax,facet=month,z=mean(tmax))) +
#     stat_summary_hex(fun = mean) +
#   facet_grid(.~month) +
#   labs(x="Year",y="Mean temperature")

```


