---
title: "HW3"
author: "James Dalgleish"
date: "October 9, 2018"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(knitr)
```
First, we'll begin by importing the dataset, cleaning the names to be more appropriate, and
focusing on the Overall Health topic (by filtering for this topic only in the BRFSS responses).
Responses are filtered to only include the 5 ordinal categories, Excellent, Very Good,
Good, Fair, Poor.
```{r data_import}
brfss <- p8105.datasets::brfss_smart2010 %>% #Pulls dataframe out of package.
  janitor::clean_names() %>%  #Converts to snake case.
  filter(topic == "Overall Health",
         response %in% c("Excellent","Very Good", "Good", "Fair", "Poor")) %>%     #Filters by overall health topic.
  mutate(response = factor(response,levels = c("Excellent","Very Good", "Good", "Fair", "Poor")))
brfss %>% 
  filter(year==2002) %>% 
  group_by(locationabbr) %>% 
count(locationabbr) %>% 
  arrange(-n)
  
```
"In 2002, which states were observed at 7 locations?""
Clearly, from the table, none were observed at exactly 7 locations.
Make a “spaghetti plot” that shows the number of locations in each state from 2002 to 2010."

"Make a table showing, for the years 2002, 2006, and 2010, the mean and standard deviation of the proportion of “Excellent” responses across locations in NY State."
The proportion of the response (in percent) is located in the data_value column, filtering by 2002,2006,2010, for NY and for Excellent responses only. The proportion therefore can be obtained by dividing by 100, from which point the mean and sd can be taken as summary statistics after grouping. 
```{r table_mean_ex}
brfss %>% 
  filter(year %in% c(2002,2006,2010),
         locationabbr == "NY") %>%
  group_by(locationdesc) %>% 
  summarize(count_ex = 
  sum(response=="Excellent"),
  count_nonex = sum(response!="Excellent"),
  count_resp= n(),
  prop_ex = count_ex/(count_nonex+count_ex)) %>% 
  select(locationdesc,prop_ex )
#across location (could mean means and sds for all regions)
location_ex_stats<-brfss %>% 
  filter(year %in% c(2002,2006,2010),
         locationabbr == "NY",response=="Excellent") %>%
  group_by(locationdesc) %>% 
  summarize(ex_sd_prop = 
  sd(data_value/100),
  ex_mean_prop = mean(data_value/100))  %>% 
    select(locationdesc,ex_sd_prop,ex_mean_prop)
#in NY state, in entirety (for completeness, question suggests a mean and sd of the proportion across locations)
statewide_ex_stats<-brfss %>% 
  filter(year %in% c(2002,2006,2010),
         locationabbr == "NY",response=="Excellent") %>%
  group_by(locationabbr) %>% 
  summarize(ex_sd_prop = 
  sd(data_value/100),
  ex_mean_prop = mean(data_value/100))  %>% 
    select(locationabbr,ex_sd_prop,ex_mean_prop)

```

For each year and state, compute the average proportion in each response category (taking the average across locations in a state). Make a five-panel plot that shows, for each response category separately, the distribution of these state-level averages over time."
Problem 2
```{r instacart_import}
instacart <- p8105.datasets::instacart %>% #Pulls dataframe out of package.
  janitor::clean_names() #cleans names
```
```{r distinct_aisles}
distinct_aisles <- instacart %>%
  group_by(aisle) %>% 
  summarize(count = n()) %>% 
  arrange(
    desc(count)
  )

```
There appear to be `r nrow(distinct_aisles)` aisles. The aisles with the most are displayed in the table below with their counts:
```{r}
distinct_aisles %>% 
  head() %>% 
  kable()
```

The numbers of items in each aisle can be visualized with the following plot:

```{r}
items_by_aisle <- instacart %>%
  group_by(aisle) %>% 
count() %>% 
  arrange(
    desc(n)
    ) %>%
as.data.frame() %>% #makes the grouping variable modifyable 
  as.tibble() %>%  #factor reorder.
  mutate(aisle = forcats::fct_reorder(aisle, n))


items_by_aisle %>% 
  ggplot(aes(x = aisle,y = n,color = n)) + #establishes mappings.
  geom_col() + #like geom_bar, but takes a variable for height.
  scale_color_viridis_c() + #adds color
  scale_y_continuous(expand=c(0,0)) + #removes whitespace at bottom.
   theme_bw() +
  theme(axis.title.x = element_blank(), #removes panel grid at back.
        axis.text.x = element_blank(),
        axis.ticks.x = element_blank(),
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) +
  labels(fill="Items in aisle") +
  ylab("Items in aisle") +
  xlab("aisles, sorted by number of items")
```

Now, we'll create a table with the most popular items in the aisles for
baking ingredients, dog food care, and packaged vegetables/fruits.
```{r}
instacart %>% 
  filter(aisle
         %in% c("baking ingredients","dog food care","packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
count(product_name) %>% 
  filter(n == max(n)) %>%
  arrange(
    desc(n)
  ) %>% 
  select(product_name,n,everything())

```

Instruction"Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table)."
Now, we'll show the mean hour of the day at which the items are ordered.
```{r}
pink_ice_tbl<-instacart %>% 
  filter(product_name %in% c("Pink Lady Apples","Coffee Ice Cream")) %>% 
  group_by(product_name,order_dow) %>% 
  summarize(mean_hour=mean(order_hour_of_day)) %>% 
  spread(key=product_name,value=,mean_hour) %>% 
mutate(order_dow = recode(.$order_dow,"0" = "Sunday",
                             "1" = "Monday",
                             "2" = "Tuesday",
                             "3" = "Wednesday",
                             "4" = "Thursday",
                             "5" = "Friday",
                             "6" = "Saturday")) %>% 
  janitor::clean_names() %>% 
  select(order_dow,
         coffee_ice_cream,
         pink_lady_apples)
pink_ice_tbl
```

Problem 3:
```{r noaa_data_import}
noaa <- p8105.datasets::ny_noaa %>% 
  as.tibble() %>% 
  janitor::clean_names()
```
"The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue. Then, do or answer the following (commenting on the results of each):"
This NOAA dataset for the new york area from `r noaa %>% distinct(id) %>% nrow()` stations has `r noaa %>% nrow` rows and `r noaa %>% ncol` columns,   which columns include total daily precipitation (prcp), miminum temperature, maximum temperature, snowfall,snowdepth at each station.
```{r missing_data}
prop.na<-function(x){
   is.na(x) %>% mean()
}
sapply(noaa,  prop.na)
```
A quick look across the column shows that there is significant missing data (which aligns with the missing data reported in the dataset description,http://p8105.com/dataset_noaa.html), almost half of the data in the maximum temperature and minimum temperature are missing.
